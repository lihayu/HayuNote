# 线性回归

线性回归（英语：linear regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。
![[../附件/9.png]]
## 线性模型：

### 基本形式：

给定由$d$个属性描述的实例$x=(x_1,x_2,\dots,x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型即是试图学习一个通过属性的线性组合来进行预测的函数，即$$f(x)=w_1+w_2+\dots+w_dx_d+b$$
$w$表示权重（weight），直观地表达了各属性在预测中地重要性
$b$表示偏置（bias），允许模型将其计算的线性超平面移开原点，从而允许模型对非零中心数据中的关系进行建模

用向量形式表示为：$$f(x)=w^Tx+b$$

##  线性回归：

给定数据集$D=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N,y_N)\}$，其中$x=(x_1,x_2,\dots,x_d)$，$y_i \in \mathbb{R}$，线性回归试图学习到$f(x_i)=wx_i+b$，使得$f(x_i) \approx y_1$。

### 学习策略：

均方误差$\sum (观测值 - 理论值)^2$是回归任务中最常用的性能度量，因此我们可以试图让均方误差达到最小化求解参数$w,b$，即
$$\begin{align}(w^*,b^*)&=\arg\min_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2 \\&=\arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_1-b)^2 \\\end{align}$$

#### 最小二乘法：

**最小二乘法**（英语：least squares method），又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。

##### 代数形式：

设$$\begin{align} E_{(w,b)}&=\sum_{i=1}^m(f(x_1)-y_i)^2\\&=\sum_{i=1}^m(y_1-wx_1-b)^2 \end{align}$$

分别对参数$w,b$求导，
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial w}&=\frac{\partial}{\partial w}\left[ \sum_{i=1}^m (y_i=wx_i-b)^2 \right]\\&=\sum_{i=1}^m\left[\frac{\partial}{\partial w} (y_i=wx_i-b)^2\right] \\&=\sum_{i=1}^m[2(y_i-wx_i-b)(-x_i)] \\&=2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right)\end{align}$$
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial b}&=\frac{\partial}{\partial b}\left[ \sum_{i=1}^m (y_i=wx_i-b)^2 \right]\\&=\sum_{i=1}^m\left[\frac{\partial}{\partial b} (y_i=wx_i-b)^2\right] \\&=\sum_{i=1}^m[2(y_i-wx_i-b)(-1)] \\&=2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right) \end{align}$$

得：
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial w}&=2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right) \\ \\ \frac{\partial E_{(w,b)}}{ \partial b}&=2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right) \end{align}$$

令其皆为$0$，可得到$w$和$b$最优解的闭式（closed-form）解

对于$w$有：
$$\because \quad 2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right)=0$$
$$\begin{align}  \therefore \quad w\sum_{i=1}^mx_i^2&=\sum_{i=1}^my_ix_i-\sum_{i=1}^mbx_i \\ &=\sum_{i=1}^my_ix_i-\sum_{i=1}^m(\bar y-w\bar x)x_i\\&=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i-w\bar x\sum_{i=1}^mx_i \end{align}$$
$$\therefore \quad w= \frac{\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx^2-\bar x\sum_{i=1}^mx_i}$$
$$\because \quad \begin{align} \bar y\sum_{i=1}^mx_i&=\frac{1}{m}\sum_{i=1}^my_i\sum_{i=1}^mx_i=\bar x\sum_{i=1}^my_i\\\\\bar x\sum_{i=1}^mx_i&=\frac{1}{m}\sum_{i=1}^mx_i\sum_{i=1}^mx_i=\frac{1}{m}\left(\sum_{i=1}^mx_i\right)^2\end{align}$$
$$\therefore \quad w=\frac{\sum_{i=1}^my_{i}(x_i-\bar x)}{\sum_{i=1}^m(x_i)^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}$$

对于$b$有：
$$2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right)=0$$
$$\therefore \quad b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)$$

整理得：
$$\begin{align} &w=\frac{\sum_{i=1}^my_{i}(x_i-\bar x)}{\sum_{i=1}^m(x_i)^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2} \\\\&b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_1) \end{align}$$

其中$\bar x=\frac{1}{m}\sum_{i=1}^m x_i$为x的均值

##### 矩阵形式：

试图学得$f(x_i)=w^Tx_1+b$，使得$f(x_i) \approx y_1$，为方便讨论，我们把向量$w$和$b$吸收进向量$\hat{w}=(w,b)$，令$X=(x_1,x_2,\dots,x_n,1)^T$，$Y=(y_1.y_2.\dots,y_n)^T$，有
	$$\begin{align}E_\hat{w}&=\lVert w^Tx_i+b-y_i \rVert^2\\&=(w^Tx_i+b-y_i)^2\\&=(\hat{w}^TX^T-Y^T)(\hat{w}^TX^T-Y^T)^T\\&=\hat{w}^TX^TX\hat{x}-\hat{w}X^TY-Y^TX\hat{w}-Y^TY\\&=\hat{w}^TX^TX\hat{x}-2\hat{w}X^TY-Y^TY\end{align}$$
所以有，
	$$\therefore \quad \hat{w}^*=\arg\min_w\left(\hat{w}^TX^TX\hat{x}-2\hat{w}X^TY-Y^TY\right)$$

对$\hat{w}$求导得，
$$\frac{\partial E_{\hat{w}}}{ \partial \hat{w}}=2X^TX\hat{w}-2X^TY$$

令其为$0$，可得到$\hat{w}$最优解
$$\begin{align} \because \quad 2X^TX\hat{w}^{*}-2X^TY &=0 \\ \therefore \quad X^TX \hat{w}^{*}&=X^TY \end{align}$$



$$\therefore \begin{align} \quad \hat{w}^*&=(X^TX)^{-1}X^TY \\&=X^\mathcal{g}Y\end{align}$$
其中$X^\mathcal{g}$为$X$的伪逆矩阵

#### 梯度下降法：

梯度下降法（英语：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。

设$$\begin{align} E_{(w,b)}&=\sum_{i=1}^m(f(x_1)-y_i)^2\\&=\sum_{i=1}^m(y_1-wx_1-b)^2 \end{align}$$
为方便运算，对式右边乘以$\frac{1}{2m}$，有

$$J{(w,b)}=\frac{1}{2m}\sum_{i=1}^m(y_1-wx_1-b)^2$$
求取参数$w,b$梯度
$$\begin{align} \triangledown _ wJ{(w,b)}&=\frac{\partial J_{(w,b)}}{ \partial w}=-\frac{1}{m}\sum_{i=1}^m[(y_i-wx_i-b)(x_i)] \\ \\ \triangledown _ bJ{(w,b)}&=\frac{\partial J_{(w,b)}}{ \partial b}=-\frac{1}{m}\sum_{i=1}^m(y_i-wx_i-b)\end{align}$$

##### 批量梯度下降：

**批量梯度下降法**是最原始的形式，它是指在**每一次迭代时**使用**所有样本**来进行梯度的更新。

更新参数，迭代直到收敛
$$\begin{align} w^*&=w+\alpha\frac{\partial J_{(w,b)}}{\partial w}=w-\alpha x_i\sum_{i=1}^m(y_i-wx_i-b)\\ \\  b^*&=b+\alpha\frac{\partial J_{(w,b)}}{ \partial b}=b-\frac{\alpha}{m}\sum_{i=1}^m(y_i-wx_i-b) \end{align}$$


##### 随机梯度下降：

**随机度下降法**是在**每次迭代时**使用**一个样本**来对参数进行更新，一旦到达最大的迭代次数或是满足预期的精度，就停止。

更新参数，直到达到最大的迭代次数或是满足预期的精度
$$\begin{align} w^*&=w+\alpha\frac{\partial J_{(w,b)}}{\partial w}=w-\alpha x_i(y_i-wx_i-b)\\ \\  b^*&=b+\alpha\frac{\partial J_{(w,b)}}{ \partial b}=b-\frac{\alpha}{m}(y_i-wx_i-b) \end{align}$$

#### 牛顿法：

牛顿法是机器学习中用的比较多的一种优化算法。牛顿法的基本思想是利用迭代点$x_K$处的一阶导数 (梯度)和二阶导数 (Hessen 矩阵) 对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。

牛顿法的速度相当快，而且能高度逼近最优值。牛顿法最突出的优点是收敛速度快，具有局部二阶收敛性，其分为基本牛顿法和全局牛顿法。

##### 基本牛顿法：

基本牛顿法是基于导数的算法，他每一步的迭代方向都是沿着当前点函数值下降的方向。对于一维的情形，对需要求解的优化函数$f(x)$，，求函数的极值的问题可以转化为求导函数$f(x)=0$，对$f(x)$进行泰勒展开到二阶，得：
$$f(x)=f(x_k)+f'(x_k)(x-x_k)+\frac{1}{2}f''(x_k)(x-x_k)^2$$

对上式求导并令其为0，则
$$f'(x_k)+f''(x_k)(x-x_k)=0$$

即得到
$$x=x_k-\frac{f'(x_k)}{f''(x_k)}$$

一、给定终止误差值$0 \leq \varepsilon \leq 1$，初始点$x_0 \in \mathbb{R}^n$，令$k=0$

二、计算$g_k = \triangledown f(x_k)$，若$\left \| g_k \right \| \leq \varepsilon$，则停止，输出$x^* \approx x_k$ 

三、计算$G_k = \triangledown ^2 f(x_k)$，并求解线性方程组$G_kd=-g_k$得解$d_k$

四、令$x_{k+1} = x_k + d_k$，$k=k+1$，并转至步骤二

##### 全局牛顿法：

牛顿法最突出的优点是收敛速度快，具有局部二阶收敛性，但是，基本牛顿法初始点需要足够“靠近”极小点，否则，有可能导致算法不收敛，此时就引入了全局牛顿法。

一、给定终止误差值$0 \leq \varepsilon \leq 1$，初始点$x_0 \in \mathbb{R}^n$，令$k=0$

二、计算$g_k = \triangledown f(x_k)$，若$\left \| g_k \right \| \leq \varepsilon$，则停止，输出$x^* \approx x_k$ 

三、计算$G_k = \triangledown ^2 f(x_k)$，并求解线性方程组$G_kd=-g_k$得解$d_k$

四、记$k$是不满足下列不等式的最小非负整数，设$m：f(x_k+\delta ^md_k)\leq f(x_k)+\sigma \delta ^mg_k^Td_k$

五、令$x_{k+1} = x_k + d_k$，$k=k+1$，并转至步骤二

##### Armijo搜索：

给定$\delta \in (0,1)$，$\sigma \in (0,0.5)$，令步长因子$\alpha _k = \delta ^{m_k}$ ，其中 m_k是满足下列不等式的最小非负整数：$$f(x_k+\delta ^md_k)\leq f(x_k)+\sigma \delta ^mg_k^Td_k$$

利用全局牛顿法求解线性回归模型假设有$m$个训练样本，其中，每个样本有$n-1个$特征，则线性回归模型的损失函数为：$$l = \frac{1}{2}\sum_{i=1}^{m}(y^i-\sum_{j=0}^{n-1}w_jx_j^i)^2$$

若是利用全局牛顿法求解线性回归模型，需要计算线性回归模型损失函数的一阶导数和二阶导数，其一阶导数为：$$\frac{\partial l}{\partial w_j} = -\sum_{i=1}^{m}[(y^i-\sum_{j=0}^{n-1}w_j\cdot x_j^i)*x_j^i]$$

损失函数的二阶导数为：$$\frac{\partial l}{\partial w_j\partial w_k} = \sum_{i=1}^{m}(x_j^i\cdot x_k^i)$$