 # 决策树
 
**决策树**（Decision tree）是*一种用于分类和回归任务的非参数监督学习算法*。 它是一种*分层树形结构*，由根节点、分支、内部节点和叶节点组成，代表了对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。

决策树模型可以认为是*if-then*规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：互斥并且完备。每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。

![[../附件/7.png]]

## 定义：

分类决策树模型是一种描述对实例进行分类的树形结构，决策树由**结点**（node）和**有向边**（directed edge）组成。结点由两种类型：**内部节点**（internal node）和**外部节点**（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

## 学习策略：

假设给定训练数据集：$$T=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N，y_N)\}$$其中$x_i=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$为输入实例（输入向量），$n$为特征个数，$y_i \in\{c_1,c_2,\dots,c_K\}$为类标记，$i=1,2,\dots,N$，$N$为样本容量。决策树学习的目标是根据给定的数据集构建一个决策树模型，使它能够对实例进行正确分类。

决策树的本质是从训练数据集中归纳出一组分类规则，我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。决策树学习用损失函数表示这一目标，损失函数通常是*正则化的极大似然函数*。决策树学习的策略是*以损失函数为目标函数的最小化*。

当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，通常采用启发式方法，近似求解这一最优化问题。

决策树学习的算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，对应着特征空间的划分，也对应着决策树的生成（构建）。

对特征选择划分而生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象，对已生成的树还需自下而上进行**剪枝**，将树变得更简单，从而使它具有更好的泛化能力。

因此，决策树学习算法通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

## 算法：

### 特征选择：

特征选择是决定用哪个特征来划分特征空间，其目的在于选取对训练数据具有分类能力的特征，通常特征选择的准则是*信息增益*和*信息增益比*。

#### 信息熵

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。

**信息熵**（information entropy）是度量样本集合纯度最常用的一种指标，可以度量随机变量$X$的不确定性。信息熵越大越不确定，可转换到度量样本集合纯度，信息熵越小样本集合的纯度越高。

设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, \quad i=1,2,\dots,n$$
则随机变量$X$的熵定义为$$Entropy=H(X)=-\sum_{i=1}^np_ilogp_i$$
在式中，若$p_i=0$，则定义$0log(0)=0$。通常，对式中的对数以2或以$e$为底（自然对数），这时熵的单位分别称作比**特**（bit）或者**纳特**（nat）。由定义可知，熵只依赖于$X$的分布，与$X$的取值无关，所以也可以将$X$的熵记作$H(p)$，即
$$H(p)=-\sum_{i=1}^n p_ilogp_i$$

熵越大，随机变量的不确定性越大。熵越大，说明系统越混乱，携带的信息就越少（为了确定XX我们需要更多的信息）；熵越小，说明系统越有序，携带的信息就越多。

从定义可验证
$$0\leqslant H(p) \leqslant n$$
当随机变量只取两个值，例如$1,0$时，即$X$的分布为
$$P(X=1)=p, \quad P(X=0)=1-p, \quad 0\leqslant p \leqslant1$$
熵为
$$H(p)=-plog_2p-(1-p)log_2(1-p)$$
![[../附件/8.png]]

当$p=0$或$p=1$时，$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H=(p)=1$，熵取值最大，随机变量不确定性最大。

设有随机变量$(X,Y)$，其联合概率分布为
$$P(X=x_i,Y=y_i)=p_{ij} ,\quad i=1,2,\dots,n; \quad j=1,2,\dots,m$$
**条件熵**（conditional entropy）$H(Y \vert X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。

随机变量$X$给定的条件下，随机变量$Y$的条件熵$H(Y\vert X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望
$$H(Y\vert X)=\sum_{i=1}^np_iH(Y\vert X=x_i)$$ 这里，$p_i=P(X=x_i),\quad i=1,2,\dots,n$。

当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别认为**经验熵**（empirical entropy）和**经验条件熵**（empirical conditional entropy）。此时，如果由$0$概率，令$0log(0)=0$。

#### 信息增益

**信息增益**（information gain）表示得知特征$X$的信息而使类$Y$的信息的不确定性减少的程度。一般而言，信息增益越大，则意味着使用该特征来划分所获得的*纯度提升*越大。

##### 定义：

特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$D(D)$与特征$A$给定条件下$D$的经验条件熵$H(D \vert A)$之差，即$$Gain=g(D,A)=H(D)-H(D \vert A)$$
一般地，熵$H(D)$与条件熵$H(D \vert A)$之差称为互信息（mutual information），它度量了$D$在知道$A$以后不确定性减少程度。决策树学习中的信息增益等价于训练数据集类与特征的互信息。
![[../附件/10.png]]
决策树学习应用信息增益准则选择特征。给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集D进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么它们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法：对训练数据集（或子集）$D$，计算器每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

##### 算法：
- 输入；训练数据集$D$和特征$A$；
- 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$;

设训练数据集为$D$，$\lvert D \rvert$表示其样本容量，即样本个数。设有$K$个类$C_k$，$k=1,2,\dots,K$，$\lvert C_k \lvert$为属于类$C_k$的样本个数，$\sum_{k=1}^K \lvert C_k \rvert= \lvert D \rvert$。设特征$A$有$n$个不同的取值$\{a_1,a_2,\dots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1.D_2,\dots,D_n$，$\lvert D_i \rvert$为$D_i$的样本个数，$\sum_{i=1}^n \lvert D_i \rvert = \lvert D \rvert$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i \cap C_k$，$\lvert D_{ik} \rvert$为$D_{ik}$的样本个数。

一、计算数据集$D$的经验熵$H(D)$
$$H(D)=-\sum_{k=1}^K \frac{\lvert C_k \rvert}{\lvert D \rvert} log_2 \frac{\lvert C_k \rvert}{\lvert D \rvert}$$

二、计算特征$A$对数据集$D$的经验条件熵$H(D \vert A)$
$$\begin{align} H(D \vert A) &= \sum_{i=1}^{n} \frac{\lvert D_i \rvert}{\lvert D \rvert} H(D_i) \\&= - \sum_{i=1}^{n} \frac{\lvert D_i \rvert}{\lvert D \rvert} \sum_{k=1}^{K} \frac{\lvert D_{ik} \rvert}{\lvert D_i\rvert} \log_2 {\frac{\lvert D_{ik}\rvert}{\lvert D_i\rvert}} \end{align}$$

三、计算信息增益$$g(D,A)=H(D)-H(D \vert A)$$

#### 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用**信息增益比**（information gain ratio）可以对这一问题进行矫正。这是特征选择的另一准则。

##### 定义：

特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的熵$H_A(D)$之比，即
$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$
其中，$$H_A(D)=-\sum_{i=1}^n \frac{\lvert D_i \rvert}{\lvert D \rvert} log_2 \frac{\lvert D_i \rvert}{\lvert D \rvert}$$
$n$是特征$A$取值的个数。


### 决策树的生成：

#### ID3算法：

D3的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树。

具体方法：从根结点（root node）开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点再对子结点递归调用以上方法。构建决策树，直到所有特征的信息增益均很小或没有特征可选为止。最后得到一颗决策树，ID3相当于用极大似然法进行概率模型的选择。

##### 算法：
- 输入：训练数据集$D$，训练集$A$和阈值$\varepsilon$；
- 输出：决策树$T$；

一、若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$

二、若$A=\oslash$，则$T$为单节点树，并将$D$中实例树最大的类$c_k$作为该结点的类标记，返回$T$

三、否则，按信息增益算法计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$
$$A_g \Leftarrow \arg\max_{\Delta} \left \lbrace \begin{align}
g(D,A_1)\\ 
g(D,A_2)\\
g(D,A_n)
\end{align}\right.$$
四、如果$A_g$的信息增益小于阈值阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$c_k$作为该结点的类标记，返回$T$

五、否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干个非空子集$D_i$，将$D$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$

六、对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用步骤一至步骤五，得到子树$T_i$，返回$T_i$

##### 特点：

- ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途
- ID3偏好选择取值多的特征做分支，ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大，取值比较多的特征，就可以分叉出更多的分支，分支更多，每个分支的纯度必然更高
- ID3算法对于缺失值的情况没有做考虑

#### C4.5算法： 

C4.5算法对ID3算法进行了改进，C4.5在生成树地过程中，用信息增益比来选择特征。

**对于连续值**
因为连续属性的可取值数目不再有限，因此不能像前面处理离散属性枚举离散属性取值来对结点进行划分。因此需要连续属性离散化，常用的离散化策略是二分法。

一、将$n$个连续值从小到大排列。给定样本集$D$和连续特征$a$，特征$a$在$D$上有$n$个不同的取值，经从小到大排列，记为$\{a_1,a_2,\dots,a_n\}$。

二、取相邻两样本值的中位点（平均值），得到$n−1$个划分点。特征$a$的第$i$个划分点记为$T_a=\frac{a^i+a^{i+1}}{2}$，然后，就可像离散特征值一样来考察这些划分点。

四、用信息增益最大的点作为最优的划分点进行样本集的划分。

五、用信息增益比来选择特征，构建决策树。

**对于缺失值**

给定训练集$D$和特征$a$，令$\tilde{D}$表示$D$中在特征$a$上没有缺失值的样本子集,可根据$\tilde{D}$来进行特征选择。

假定特征$a$有$V$个可取值$\{a^1,a^2,\dots,a^V\}$，令$\tilde{D}^v$表示$\tilde{D}$中在特征$a$上取值为$a^v$的样本子集，$\tilde{D}^k$表示$\tilde{D}^v$中第$k$类$(k=1,2,\dots,\lvert Y \rvert)$的样本子集，假定为每个样本$x$赋予一个权重$\omega_x$，并定义

$$\begin{align}
& \rho = \frac{\sum_{x \in \tilde{D}} \omega_x}{\sum_{x \in D} \omega_x} \\ \\
& \tilde{p}_k = \frac{\sum_{x \in \tilde{D}_k} \omega_x}{\sum_{x \in D} \omega_x} \\ \\
& \tilde{r}_v = \frac{\sum_{x \in \tilde{D}^v} \omega_x}{\sum_{x \in D} \omega_x}
\end{align}$$

对特征$a$，$\rho$表示无缺失值样本所占的比例，$\widetilde{\rho}_k$表示无缺失值样本中第$k$类所占的比例，$\tilde{\rho}^v$ 表示无缺失值样本中在属性$a$上取值$a^v$的样本所占的比例。显然，$$\sum_{k=1}^{\lvert \mathcal{Y} \rvert} \widetilde{\rho}_k=1,\quad \sum_{v=1}^V \widetilde{r}_v=1$$

此时信息增益：
$$\begin{align}
g(D,a)
& = \rho \times g(\tilde{D}, a) \\
& = \rho \times (H(\tilde{D}) - \sum_{v=1}^{V} \tilde{r}_v H(\tilde{D}^v)) \\
H(\tilde{D})
& = -\sum_{k=1}^{\mathcal{|Y|}} \tilde{p}_k log_2 \tilde{p}_k
\end{align}$$

若样本$x$的划分特征已知，则将$x$划入该特征对应的子结点，且样本权值在子结点中保持为$ω_x$。

若样本$x$的划分特征未知，则将$x$划入所有子结点，且样本权值在各个子结点中调整为$\tilde{r}_v⋅ω_x$，这就让同一样本以不同的概率划入到不同的子结点中。

##### 算法：
- 输入：训练数据集$D$，训练集$A$和阈值$\varepsilon$；
- 输出：决策树$T$；

一、若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$。

二、若$A=\oslash$，则$T$为单节点树，并将$D$中实例树最大的类$c_k$作为该结点的类标记，返回$T$。

三、否则，按信息增益比算法计算$A$中各特征对$D$的信息增益比，选择信息增益最大的特征$A_g$
$$A_g \Leftarrow \arg\max_{\Delta} \left \lbrace \begin{align}
g(D,A_1)\\ 
g(D,A_2)\\
g(D,A_n)
\end{align}\right.$$
四、如果$A_g$的信息增益小于阈值阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$c_k$作为该结点的类标记，返回$T$

五、否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干个非空子集$D_i$，将$D$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$

六、对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用步骤一至步骤五，得到子树$T_i$，返回$T_i$

##### 特点：

- 对与ID3没有考虑连续特征，选择信息增益最大的点作为该连续特征的二元离散分类点，做到了连续特征的离散化。
- 对与ID3偏好选择取值多的特征做分，用信息增益率作为分支指标，因为特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。
- 对与ID3算法对于缺失值的情况没有做考虑，主要解决了两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。  
- 对与ID3没有考虑过拟合的问题，C4.5引入了正则化系数进行初步的剪枝

### 决策树的剪枝：

在决策树学习中将已生成的树进行简化的过程称为**剪枝**（pruning）。

具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点从而简化分类树模型，提高模型的泛化能力。决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。

#### 学习策略：

设树$T$的叶结点个数为$\lvert T \rvert$，$t$是树$T$的叶结点，该叶结点有$N_i$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\dots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha \geqslant 0$为参数，则决策树的损失函数可以定义为$$C_\alpha(T)=\sum_{t=1}^{\lvert T \rvert}N_tH_t(T)+\alpha \vert T\rvert$$
其中经验熵为$$H_t(T)=-\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$
在损失函数中，式$C_\alpha(T)$第1项记作$$\begin{align}C(T) = \sum_{t=1}^{\lvert T \rvert} N_t H_t(T) &= \sum_{t=1}^{\lvert T\rvert} N_t [- \sum_{k=1}^{K} \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}] \\ &= - \sum_{t=1}^{\lvert T\rvert} \sum_{k=1}^{K} N_{tk}  \log \frac{N_{tk}}{N_t}\end{align}$$
这时有$$C_\alpha(T)=C(T)+\alpha \lvert T \rvert$$
其中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\lvert T \rvert$表示模型的复杂度，$\alpha \geqslant 0$控制两者之间的影响。

较大的$\alpha$促使选择较简单的模型（树），较小的$\alpha$促使选择较复杂的模型（树）。$\alpha=0$意味着只需考虑模型的拟合程度，不许考虑模型的复杂度。

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数，还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

##### 算法：
- 输入：生成算法产生的整个$T$，参数$\alpha$
- 输出：修剪后的子树$T_a$

一、计算每个结点的经验熵

二、递归地从树的叶结点向上回缩

设一组设一组叶结点回缩到其父结点之前与之后的整体树为$T_B$与$T_A$，其对于的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$，如果$$C_\alpha(T_A) \geqslant C_\alpha(T_B)$$则剪枝，将父结点变成新的叶结点。

三、返回步骤二，直至不能继续为止，得到损失函数最小的子树$T_\alpha$

## CART 算法：

分类与回归树（classification and regression tree，CART）模型，由breiman等人在1984年提出，是应用广泛的决策树学习方法，CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。

CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。CART算法分为两步：

一、决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。  

二、决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### CART 生成：

决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。

#### 回归树的生成：

##### 学习策略：

假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$$D=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N,y_N)\}$$考虑如何生成回归树。

一颗回归树对应着输入空间即（特征空间）的一个划分以及在划分的单元上输出的值。假设已将输入空间划分为$M$个单元$R_1,R_2.\dots,R_M$，并且在每个单元$R_m$，上有一个固定的输出值$c_m$，于是回归树模型可表示为：
$$f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)$$
当输入空间划分时可以用平方误差$\sum_{x_i \in R_m} (y_i - f(x_i))^2$来表示训练误差，用最小化平方误差来求解。而单元$R_m$上的$c_m$的最优值$c_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：
$$\hat{c}_m = ave(y_i  \vert  x_i \in R_m)$$
这里采用启发式的方法对输入空间进行划分，选择$j$个变量$x^(j)$和它的取值$s$，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：
$$R_1(j, s) = \{x \vert x^{(j)} \leqslant s\}, \ R_2(j, s) = \{x \vert  x^{(j)} > s\}$$
然后寻找最优切分变量$j$和最优切分点$s$，具体地，求解：
$$\min_{j, s} \ [ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2]$$

对固定输入变量$j$可以找到最优切分点$s$。
$$\hat{c}_1 = ave( y_i  \vert  x_i \in R_1(j, s)), \quad \hat{c}_2 = ave( y_i  \vert  x_i \in R_2(j, s))$$
遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$。依次将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止，这样的回归树通常称为最小二乘回归树（least squares regression tree）。

##### 算法：
- 输入：训练数据集$D$
- 输出：回归树$f(x)$

在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

一、选择最优切分变量$j$与切分点$s$，求解
$$\min_{j,s}\left[\min_{c_1}\sum_{x_i \in R_1(j,s)}(y_1-c_1)^2+\min_{c_1}\sum_{x_i \in R_2(j,s)}(y_1-c_2)^2\right]$$

遍历变量$j$，对固定的且分辨率$j$扫描切分点$s$，选择使式达到最小值的$(j,s)$

二、用选定的对$(j,s)$划分区域并决定输出值
$$R_1(j,s)=\{x\vert x^{(j)}\leqslant s\}, \quad R_1(j,s)=\{x\vert x^{(j)}>s\}$$
$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i \in R_M(j,s)}y_i, \quad x \in R_m,\quad m=1,2$$

三、继续对两个子区域调用步骤一、二，直至满足停止条件

四、将输入空间划分为$M$个区域，$R_1,R_2,\dots,R_M$，生成决策树
$$f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)$$

#### 分类树的生成：

##### 基尼指数：

分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为
$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$

对于二类分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为
$$Gini(p)=2p(1-p)$$

对于给定的样本集合$D$，$D$中属于第$k$类的样本子集为$C_k$，类的个数为$K$，则其基尼指数为
$$Gini(D)=1-\sum_{k=1}^K\left(\frac{\lvert C_k \rvert}{\lvert D\rvert} \right)^2$$

如果样本集合$D$根据特征$A$是否取某一可能取值$a$被分割成$D_1$和$D_2$两部分，即
$$D_1=\{(x,y)\in D\vert A(x)=a \},\quad D_2=D-D_1$$

则在特征$A$的条件下，集合$D$的基尼指数定义为
$$Gini(D)=\frac{\lvert D_1 \rvert}{\lvert D\rvert}Gini(D_i)+\frac{\lvert D_2 \rvert}{\lvert D\rvert}Gini(D_2)$$

基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点和熵相似。

基尼指数反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此基尼指数越小，则数据集纯度越高。基尼指数偏向于特征值较多的特征，类似信息增益。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等。

##### 算法：
- 输入：训练数据集$D$，停止计算的条件
- 输出：CART决策树

根据训练数据集，从根结点开始，递归地对每个结点进行一下操作，构建二叉决策树：

一、设结点的训练数据集为$D$,计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试是“是”或“否”将$D$分割成$D_1$和$D_2$两部分

二、在所有可能的特征A以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中

三、对两个子结点递归地调用步骤一，二，直至满足停止条件

四、生成CART决策树

  算法停止的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。

### CART剪枝：

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：首先，从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$T_0,T_1,\dots,T_n$；然后，通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。

##### 学习策略：

一、剪枝形成一个子树序列

在剪枝过程中，计算子树的损失函数:
$$C_\alpha(T)=C(T)+α|T|$$
其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼指数），$\lvert T \rvert$为子树的叶结点个数，$\alpha \geqslant 0$为参数，$C_\alpha(T)$为参数是$\alpha$时子树$\lvert T \rvert$的整体损失，参数$\alpha$权衡训练数据的拟合程度与模型的复杂度。

对固定的$\alpha$，一定存在使损失函数$C_\alpha(T)$最小的子树，将其表示为$T_\alpha$。$T_\alpha$在损失函数$C_\alpha(T)$最小的意义下时最优的。容易验证这样的最优子树是唯一的。当$\alpha$大的时候，最优子树$T_\alpha$偏小；当$\alpha$小的时候，最优子树$T_\alpha$偏大。极端情况，当$\alpha=0$的时候，整体树是最优的。当$\alpha \to \infty$的时候，根节点组成的单结点树是最优的。

经证明，可以用递归的方法对树进行剪枝。将$\alpha$从小增大，$0=\alpha < a_1<\dots<a_n<\infty$，产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,\dots,n$；剪枝得到的子树序列对应着区间$\alpha \in [\alpha_i,\alpha_{i+1}),i=1,2,\dots,n$的最优字数序列$\{T_0,T_1,\dots,T_n\}$，序列中的子树的嵌套的。

具体地，从整体书$T_0$开始剪枝，对$T_0$的任意内部节点$t$，以$t$为单结点树的损失函数是
$$C_\alpha(t)=C(t)+α$$

以$t$为根结点的子树$T_t$的损失函数是
$$C_\alpha(T_t)=C(T_t)+α|T_t|$$

当$a=0$及$a$充分小时，有不等式
$$C_\alpha(T_t)<C(T_t)$$

当$a$增大时，在某一$a$，有
$$$C_\alpha(T_t)=C(T_t)$$

当$a$再增大时，不等式$C_\alpha(T_t)<C(T_t)$反向。只要$\alpha=\frac{C(t)-C(T_t)}{\lvert T_t \rvert-1}$，$T_t$和$t$有相同的损失函数，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。

为此，对$T_0$中每一内部节点$t$，计算
$$g(t)=\frac{C(t)-C(T_t)}{\lvert T_t \rvert-1}$$

它表示剪枝后整体损失函数减少的程度。在$T_0$中减去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$a_1$。$T_1$为区间$[a_1,a_2)$的最优子树。

如此剪枝下去，直至得到根节点。在这过程中，不断地增加$\alpha$的值，产生新的区间。

二、在剪枝得到的子树序列$T_0,T_1,\dots,T_n$中通过交叉验证选取最优的子树$T_\alpha$

具体地，利用独立的验证数据集，测试子树序列为$T_0,T_1,\dots,T_n$中各棵子树中的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$T_0,T_1,\dots,T_n$都对应着一个参数$\alpha_0,\alpha_1,\dots,\alpha_n$。所以，当最优子树$T_k$确定时，对应的$a_k$也确定了，即得到最优决策树$T_\alpha$。

##### 算法：
- 输入：CART算法生成的决策树$T_0$
- 输出：最优决策树$T_\alpha$

一、设$k=0,T=T_0$

二、设$\alpha = +\infty$

三、自下而上地对各内部结点$t$计算$C(T_t),\lvert T_t \rvert$以及
$$\begin{align} g(t)&=\frac{C(t)-C(T_t)}{\lvert T_t \rvert-1}\\\\\alpha&=min(\alpha,g(t)) \end{align}$$

这里，$T_t$表示以$t$为根结点地子树，$C(T_t)$是对训练数据地预测误差，$\lvert T_t \rvert$是$T_t$地叶结点个数

四、对$g(t)=\alpha$地内部结点$t$进行剪枝，并对叶结点$t$以多数表决发决定其类，得到树$T$

五、设$k=k+1,a_k=a,T_k=T$

六、如果$T_k$不是由根结点及两个叶结点构成地树，回到步骤二，否则令$T_k=T_n$

七、采用交叉验证法在子树序列$T_0,T_1,\dots,T_n$中选取最优子树$T_\alpha$