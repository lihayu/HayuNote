 # 决策树
 
**决策树**（Decision tree）是*一种用于分类和回归任务的非参数监督学习算法*。 它是一种*分层树形结构*，由根节点、分支、内部节点和叶节点组成，代表了对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。

决策树模型可以认为是*if-then*规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：互斥并且完备。每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。

![[../附件/7.png]]

## 定义：

分类决策树模型是一种描述对实例进行分类的树形结构，决策树由**结点**（node）和**有向边**（directed edge）组成。结点由两种类型：**内部节点**（internal node）和**外部节点**（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。

## 学习策略：

假设给定训练数据集：$$T=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N，y_N)\}$$其中$x_i=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$为输入实例（输入向量），$n$为特征个数，$y_i \in\{c_1,c_2,\dots,c_K\}$为类标记，$i=1,2,\dots,N$，$N$为样本容量。决策树学习的目标是根据给定的数据集构建一个决策树模型，使它能够对实例进行正确分类。

决策树的本质是从训练数据集中归纳出一组分类规则，我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。决策树学习用损失函数表示这一目标，损失函数通常是*正则化的极大似然函数*。决策树学习的策略是*以损失函数为目标函数的最小化*。

当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，通常采用启发式方法，近似求解这一最优化问题。

决策树学习的算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，对应着特征空间的划分，也对应着决策树的生成（构建）。

对特征选择划分而生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象，对已生成的树还需自下而上进行**剪枝**，将树变得更简单，从而使它具有更好的泛化能力。

因此，决策树学习算法通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪。

## 算法：

### 特征选择：

特征选择是决定用哪个特征来划分特征空间，其目的在于选取对训练数据具有分类能力的特征，通常特征选择的准则是*信息增益*和*信息增益比*。

#### 信息熵

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。

**信息熵**（information entropy）是度量样本集合纯度最常用的一种指标，可以度量随机变量$X$的不确定性。信息熵越大越不确定，可转换到度量样本集合纯度，信息熵越小样本集合的纯度越高。

设$X$是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, \quad i=1,2,\dots,n$$
则随机变量$X$的熵定义为$$H(X)=-\sum_{i=1}^np_ilogp_i$$
在式中，若$p_i=0$，则定义$0log(0)=0$。通常，对式中的对数以2或以$e$为底（自然对数），这时熵的单位分别称作比**特**（bit）或者**纳特**（nat）。由定义可知，熵只依赖于$X$的分布，与$X$的取值无关，所以也可以将$X$的熵记作$H(p)$，即$$H(p)=-\sum_{i=1}^n p_ilogp_i$$
熵越大，随机变量的不确定性越大。

从定义可验证
$$0\leqslant H(p) \leqslant n$$
当随机变量只取两个值，例如$1,0$时，即$X$的分布为
$$P(X=1)=p, \quad P(X=0)=1-p, \quad 0\leqslant p \leqslant1$$
熵为
$$H(p)=-plog_2p-(1-p)log_2(1-p)$$
![[../附件/8.png]]

当$p=0$或$p=1$时，$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H=(p)=1$，熵取值最大，随机变量不确定性最大。

设有随机变量$(X,Y)$，其联合概率分布为
$$P(X=x_i,Y=y_i)=p_{ij} ,\quad i=1,2,\dots,n; \quad j=1,2,\dots,m$$
**条件熵**（conditional entropy）$H(Y \vert X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。

随机变量$X$给定的条件下，随机变量$Y$的条件熵$H(Y\vert X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望
$$H(Y\vert X)=\sum_{i=1}^np_iH(Y\vert X=x_i)$$ 这里，$p_i=P(X=x_i),\quad i=1,2,\dots,n$。

当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别认为**经验熵**（empirical entropy）和**经验条件熵**（empirical conditional entropy）。此时，如果由$0$概率，令$0log(0)=0$。

#### 信息增益

**信息增益**（information gain）表示得知特征$X$的信息而使类$Y$的信息的不确定性减少的程度。一般而言，信息增益越大，则意味着使用该特征来划分所获得的*纯度提升*越大。

##### 定义：

特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$D(D)$与特征$A$给定条件下$D$的经验条件熵$H(D \vert A)$之差，即$$g(D,A)=H(D)-H(D \vert A)$$
一般地，熵$H(Y)$与条件熵$H(D \vert A)$之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集类与特征的互信息。

决策树学习应用信息增益准则选择特征。给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集D进行分类的不确定性。而经验条件熵$H(D|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么它们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法：对训练数据集（或子集）$D$，计算器每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

##### 算法：
- 输入；训练数据集$D$和特征$A$；
- 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$;

设训练数据集为$D$，$\lvert D \rvert$表示其样本容量，即样本个数。设有$K$个类$C_k$，$k=1,2,\dots,K$，$\lvert C_k \lvert$为属于类$C_k$的样本个数，$\sum_{k=1}^K \lvert C_k \rvert= \lvert D \rvert$。设特征$A$有$n$个不同的取值$\{a_1,a_2,\dots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1.D_2,\dots,D_n$，$\lvert D_i \rvert$为$D_i$的样本个数，$\sum_{i=1}^n \lvert D_i \rvert = \lvert D \rvert$。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i \cap C_k$，$\lvert D_{ik} \rvert$为$D_{ik}$的样本个数。

一、计算数据集$D$的经验熵$H(D)$
$$H(D)=-\sum_{k=1}^K \frac{\lvert C_k \rvert}{\lvert D \rvert} log_2 \frac{\lvert C_k \rvert}{\lvert D \rvert}$$
二、计算特征$A$对数据集$D$的经验条件熵$H(D \vert A)$
$$\begin{align} H(D \vert A) &= \sum_{i=1}^{n} \frac{\lvert D_i \rvert}{\lvert D \rvert} H(D_i) \\&= - \sum_{i=1}^{n} \frac{\lvert D_i \rvert}{\lvert D \rvert} \sum_{k=1}^{K} \frac{\lvert D_{ik} \rvert}{\lvert D_i\rvert} \log_2 {\frac{\lvert D_{ik}\rvert}{\lvert D_i\rvert}} \end{align}$$
三、计算信息增益$$g(D,A)=H(D)-H(D \vert A)$$
#### 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用**信息增益比**（information gain ratio）可以对这一问题进行矫正。这是特征选择的另一准则。

##### 定义：

特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的熵$H_A(D)$之比，即
$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$
其中，$$H_A(D)=-\sum_{i=1}^n \frac{\lvert D_i \rvert}{\lvert D \rvert} log_2 \frac{\lvert D_i \rvert}{\lvert D \rvert}$$
$n$是特征$A$取值的个数。

### 决策树的生成：

#### ID3算法：

D3的核心是在决策树各个结点上应用信息增益准则选择特征，递归的构建决策树。

具体方法：从根结点（root node）开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点再对子结点递归调用以上方法。构建决策树，直到所有特征的信息增益均很小或没有特征可选为止。最后得到一颗决策树，ID3相当于用极大似然法进行概率模型的选择。

##### 算法：
- 输入：训练数据集$D$，训练集$A$和阈值$\varepsilon$；
- 输出：决策树$T$；

一、若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$

二、若$A=\oslash$，则$T$为单节点树，并将$D$中实例树最大的类$c_k$作为该结点的类标记，返回$T$

三、否则，按信息增益算法计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$
$$A_g \Leftarrow \arg\max_{\Delta} \left \lbrace \begin{align}
g(D,A_1)\\ 
g(D,A_2)\\
g(D,A_n)
\end{align}\right.$$
四、如果$A_g$的信息增益小于阈值阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$c_k$作为该结点的类标记，返回$T$

五、否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干个非空子集$D_i$，将$D$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$

六、对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用步骤一至步骤五，得到子树$T_i$，返回$T_i$

#### C4.5算法：

C4.5算法对ID3算法进行了改进，C4.5在生成树地过程中，用信息增益比来选择特征。

##### 算法：
- 输入：训练数据集$D$，训练集$A$和阈值$\varepsilon$；
- 输出：决策树$T$；

一、若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$

二、若$A=\oslash$，则$T$为单节点树，并将$D$中实例树最大的类$c_k$作为该结点的类标记，返回$T$

三、否则，按信息增益比算法计算$A$中各特征对$D$的信息增益比，选择信息增益最大的特征$A_g$
$$A_g \Leftarrow \arg\max_{\Delta} \left \lbrace \begin{align}
g(D,A_1)\\ 
g(D,A_2)\\
g(D,A_n)
\end{align}\right.$$
四、如果$A_g$的信息增益小于阈值阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$c_k$作为该结点的类标记，返回$T$

五、否则，对$A_g$的每一个可能值$a_i$，依$A_g=a_i$，将$D$分割为若干个非空子集$D_i$，将$D$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$

六、对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归地调用步骤一至步骤五，得到子树$T_i$，返回$T_i$

### 决策树的剪枝：

在决策树学习中将已生成的树进行简化的过程称为**剪枝**（pruning）。

具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点从而简化分类树模型，提高模型的泛化能力。决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。

#### 学习策略：

设树$T$的叶结点个数为$\lvert T \rvert$，$t$是树$T$的叶结点，该叶结点有$N_i$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\dots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha \geqslant 0$为参数，则决策树的损失函数可以定义为$$C_\alpha(T)=\sum_{t=1}^{\lvert T \rvert}N_tH_t(T)+\alpha \vert T\rvert$$
其中经验熵为$$H_t(T)=-\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$
在损失函数中，式$C_\alpha(T)$第1项记作$$\begin{align}C(T) = \sum_{t=1}^{\lvert T \rvert} N_t H_t(T) &= \sum_{t=1}^{\lvert T\rvert} N_t [- \sum_{k=1}^{K} \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}] \\ &= - \sum_{t=1}^{\lvert T\rvert} \sum_{k=1}^{K} N_{tk}  \log \frac{N_{tk}}{N_t}\end{align}$$
这时有$$C_\alpha(T)=C(T)+\alpha \lvert T \rvert$$
其中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$\lvert T \rvert$表示模型的复杂度，$\alpha \geqslant 0$控制两者之间的影响。

较大的$\alpha$促使选择较简单的模型（树），较小的$\alpha$促使选择较复杂的模型（树）。$\alpha=0$意味着只需考虑模型的拟合程度，不许考虑模型的复杂度。

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数，还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

##### 算法：
- 输入：生成算法产生的整个$T$，参数$\alpha$
- 输出：修剪后的子树$T_a$

一、计算每个结点的经验熵

二、递归地从树的叶结点向上回缩

设一组设一组叶结点回缩到其父结点之前与之后的整体树为$T_B$与$T_A$，其对于的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$，如果$$C_\alpha(T_A) \geqslant C_\alpha(T_B)$$则剪枝，将父结点变成新的叶结点。

三、返回步骤二，直至不能继续为止，得到损失函数最小的子树$T_\alpha$

## CART 算法：

分类与回归树（classification and regression tree，CART）模型，由breiman等人在1984年提出，是应用广泛的决策树学习方法，CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。

CART是在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。CART算法分为两步：

一、决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。  

二、决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### CART 生成：

决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。

#### 回归树的生成：

##### 学习策略：

假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集$$D=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N,y_N)\}$$考虑如何生成回归树。

一颗回归树对应着输入空间即（特征空间）的一个划分以及在划分的单元上输出的值。假设已将输入空间划分为$M$个单元$R_1,R_2.\dots,R_M$，并且在每个单元$R_m$，上有一个固定的输出值$c_m$，于是回归树模型可表示为：
$$f(x) = \sum_{m=1}^{M} c_m I(x \in R_m)$$
当输入空间划分时可以用平方误差$\sum_{x_i \in R_m} (y_i - f(x_i))^2$来表示训练误差，用最小化平方误差来求解。而单元$R_m$上的$c_m$的最优值$c_m$是$R_m$上所有输入实例$x_i$对应的输出$y_i$的均值，即：
$$\hat{c}_m = ave(y_i  \vert  x_i \in R_m)$$
这里采用启发式的方法对输入空间进行划分，选择$j$个变量$x^(j)$和它的取值$s$，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：
$$R_1(j, s) = \{x \vert x^{(j)} \leqslant s\}, \ R_2(j, s) = \{x \vert  x^{(j)} > s\}$$
然后寻找最优切分变量$j$和最优切分点$s$，具体地，求解：
$$\min_{j, s} \ [ \min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i-c_2)^2]$$

对固定输入变量$j$可以找到最优切分点$s$。
$$\hat{c}_1 = ave( y_i  \vert  x_i \in R_1(j, s)), \quad \hat{c}_2 = ave( y_i  \vert  x_i \in R_2(j, s))$$
遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$。依次将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止，这样的回归树通常称为最小二乘回归树（least squares regression tree）。

##### 算法：
- 输入：训练数据集$D$
- 输出：回归树$f(x)$

在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

一、选择最优切分变量$j$与切分点$s$，求解
$$\min_{j,s}\left[\min_{c_1}\sum_{x_i \in R_1(j,s)}(y_1-c_1)^2+\min_{c_1}\sum_{x_i \in R_2(j,s)}(y_1-c_2)^2\right]$$

遍历变量$j$，对固定的且分辨率$j$扫描切分点$s$，选择使式达到最小值的$(j,s)$

二、用选定的对$(j,s)$划分区域并决定输出值
$$R_1(j,s)=\{x\vert x^{(j)}\leqslant s\}, \quad R_1(j,s)=\{x\vert x^{(j)}>s\}$$
$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i \in R_M(j,s)}y_i, \quad x \in R_m,\quad m=1,2$$

三、继续对两个子区域调用步骤一、二，直至满足停止条件

四、将输入空间划分为$M$个区域，$R_1,R_2,\dots,R_M$，生成决策树
$$f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)$$

#### 分类树的生成：

##### 基尼指数：

分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为
$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$

对于二类分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为
$$Gini(p)=2p(1-p)$$

对于给定的样本集合$D$，$D$中属于第$k$类的样本子集为$C_k$，类的个数为$K$，则其基尼指数为
$$Gini(D)=1-\sum_{k=1}^K\left(\frac{\lvert C_k \rvert}{\lvert D\rvert} \right)^2$$

如果样本集合$D$根据特征$A$是否取某一可能取值$a$被分割成$D_1$和$D_2$两部分，即
$$D_1=\{(x,y)\in D\vert A(x)=a \},\quad D_2=D-D_1$$

则在特征$A$的条件下，集合$D$的基尼指数定义为
$$Gini(D)=\frac{\lvert D_1 \rvert}{\lvert D\rvert}Gini(D_i)+\frac{\lvert D_2 \rvert}{\lvert D\rvert}Gini(D_2)$$

基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A=a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点和熵相似。

