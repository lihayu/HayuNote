# 线性回归

线性回归（英语：linear regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

## 线性模型：

### 基本形式：

给定由$d$个属性描述的实例$x=(x_1,x_2,\dots,x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型即是试图学习一个通过属性的线性组合来进行预测的函数，即$$f(x)=w_1+w_2+\dots+w_dx_d+b$$
$w$表示权重（weight），直观地表达了各属性在预测中地重要性
$b$表示偏置（bias），允许模型将其计算的线性超平面移开原点，从而允许模型对非零中心数据中的关系进行建模

用向量形式表示为：$$f(x)=w^Tx+b$$

##  多元线性回归：

给定数据集$D=\{ (x_1，y_1),(x_2，y_2),\dots,(x_N,y_N)\}$，其中$x=(x_1,x_2,\dots,x_d)$，$y_i \in \mathbb{R}$，线性回归试图学习到$f(x_i)=wx_i+b$，使得$f(x_i) \approx y_1$。

### 学习策略：

#### 最小二乘法：

**最小二乘法**（英语：least squares method），又称最小平方法，是一种数学优化建模方法。它通过最小化误差的平方和寻找数据的最佳函数匹配。

均方误差$\sum (观测值 - 理论值)^2$是回归任务中最常用的性能度量，因此我们可以试图让均方误差达到最小化求解参数$w,b$，即
$$\begin{align}(w^*,b^*)&=\arg\min_{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2 \\&=\arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_1-b)^2 \\\end{align}$$

##### 代数形式：

设$$\begin{align} E_{(w,b)}&=\sum_{i=1}^m(f(x_1)-y_i)^2\\&=\sum_{i=1}^m(y_1-wx_1-b)^2 \end{align}$$

分别对参数$w,b$求导，
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial w}&=\frac{\partial}{\partial w}\left[ \sum_{i=1}^m (y_i=wx_i-b)^2 \right]\\&=\sum_{i=1}^m\left[\frac{\partial}{\partial w} (y_i=wx_i-b)^2\right] \\&=\sum_{i=1}^m[2(y_i-wx_i-b)(-x_i)] \\&=2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right)\end{align}$$
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial b}&=\frac{\partial}{\partial b}\left[ \sum_{i=1}^m (y_i=wx_i-b)^2 \right]\\&=\sum_{i=1}^m\left[\frac{\partial}{\partial b} (y_i=wx_i-b)^2\right] \\&=\sum_{i=1}^m[2(y_i-wx_i-b)(-1)] \\&=2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right) \end{align}$$

得：
$$\begin{align} \frac{\partial E_{(w,b)}}{ \partial w}&=2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right) \\ \\ \frac{\partial E_{(w,b)}}{ \partial b}&=2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right) \end{align}$$

令其皆为$0$，可得到$w$和$b$最优解的闭式（closed-form）解

对于$w$有：
$$\because \quad 2\left( w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right)=0$$
$$\begin{align}  \therefore \quad w\sum_{i=1}^mx_i^2&=\sum_{i=1}^my_ix_i-\sum_{i=1}^mbx_i \\ &=\sum_{i=1}^my_ix_i-\sum_{i=1}^m(\bar y-w\bar x)x_i\\&=\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i-w\bar x\sum_{i=1}^mx_i \end{align}$$
$$\therefore \quad w= \frac{\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx^2-\bar x\sum_{i=1}^mx_i}$$
$$\because \quad \begin{align} \bar y\sum_{i=1}^mx_i&=\frac{1}{m}\sum_{i=1}^my_i\sum_{i=1}^mx_i=\bar x\sum_{i=1}^my_i\\\\\bar x\sum_{i=1}^mx_i&=\frac{1}{m}\sum_{i=1}^mx_i\sum_{i=1}^mx_i=\frac{1}{m}\left(\sum_{i=1}^mx_i\right)^2\end{align}$$
$$\therefore \quad w=\frac{\sum_{i=1}^my_{i}(x_i-\bar x)}{\sum_{i=1}^m(x_i)^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2}$$

对于$b$有：
$$2\left( mb-\sum_{i=1}^m(y_i-wx_i)\right)=0$$
$$\therefore \quad b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)$$

整理得：
$$\begin{align} &w=\frac{\sum_{i=1}^my_{i}(x_i-\bar x)}{\sum_{i=1}^m(x_i)^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2} \\\\&b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_1) \end{align}$$

其中$\bar x=\frac{1}{m}\sum_{i=1}^m x_i$为x的均值

##### 矩阵形式：

试图学得$f(x_i)=w^Tx_1+b$，使得$f(x_i) \approx y_1$，为方便讨论，我们把向量$w$和$b$吸收进向量$\hat{w}=(w,b)$，令$X=(x_1,x_2,\dots,x_n,1)^T$，$Y=(y_1.y_2.\dots,y_n)^T$，有
	$$\begin{align}E_\hat{w}&=\lVert w^Tx_i+b-y_i \rVert^2\\&=(w^Tx_i+b-y_i)^2\\&=(\hat{w}^TX^T-Y^T)(\hat{w}^TX^T-Y^T)^T\\&=\hat{w}^TX^TX\hat{x}-\hat{w}X^TY-Y^TX\hat{w}-Y^TY\\&=\hat{w}^TX^TX\hat{x}-2\hat{w}X^TY-Y^TY\end{align}$$
所以有，
	$$\therefore \quad \hat{w}^*=\arg\min_w\left(\hat{w}^TX^TX\hat{x}-2\hat{w}X^TY-Y^TY\right)$$

对$\hat{w}$求导得，
$$\frac{\partial E_{\hat{w}}}{ \partial \hat{w}}=2X^TX\hat{w}-2X^TY$$

令其为$0$，可得到$\hat{w}$最优解
$$\begin{align} \because \quad 2X^TX\hat{w}^*-2X^TY&=0 \\ \therefore \quad X^TX\hat{w}^*&=X^TY \end{align}$$



$$\therefore \begin{align} \quad \hat{w}^*&=(X^TX)^{-1}X^TY \\&=X^\mathcal{g}Y\end{align}$$
其中$X^\mathcal{g}$为$X$的伪逆矩阵

